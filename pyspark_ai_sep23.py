# -*- coding: utf-8 -*-
"""Pyspark_AI_sep23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yrTXInnmLShgvAihQB_U2Ft1_BUPg9WR
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf

# create the session
conf = SparkConf().set("spark.ui.port", "4050")

# create the context
import pyspark
sc = pyspark.SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip
get_ipython().system_raw('./ngrok http 4050 &')
!sleep 10
!curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"

!pyspark --version

"""RDD word count program in pyspark"""

from google.colab import files
files.upload()

rdd = spark.sparkContext.textFile("word.txt")

rdd.collect()

rdd1 = rdd.flatMap(lambda x: x.split(" "));

rdd1.collect()

rdd2 = rdd1.map(lambda x: (x,1));

rdd2.collect()

rdd3 = rdd2.reduceByKey(lambda a,b:a+b)

rdd3.collect()

"""Dataframes in Pyspark"""

dataset = [("James", "Sales", 3000),
    ("Michael", "Sales", 4600),
    ("Robert", "Sales", 4100),
    ("Maria", "Finance", 3000),
    ("James", "Sales", 3000),
    ("Scott", "Finance", 3300),
    ("Jen", "Finance", 3900),
    ("Jeff", "Marketing", 3000),
    ("Kumar", "Marketing", 2000),
    ("Saif", "Sales", 4100)
  ]
columns= ["employee_name", "department", "salary"]

df = spark.createDataFrame(data=dataset, schema=columns)

df.show()

#remove duplicate tuples
distinctdf = df.distinct()
distinctdf.show()

#remove tuple with respect to one or more columns
dropcolumns_df = df.dropDuplicates(["department","salary"])
dropcolumns_df.show()

df.orderBy("department").show()

#running sql query on dataframe
df.createOrReplaceTempView("emp")
spark.sql("select employee_name,salary from emp order by salary").show()

df.show()

df.select("employee_name","department").show()

from pyspark.sql.functions import *

df2 = df.withColumn("salary",col("salary").cast("INTEGER"))

df2.printSchema()

df3 = df.withColumn("salary",col("salary")*100)
df3.show()

df.show()

df4 = df.withColumn("new_column",col("salary")*2)
df4.show()

df5 = df.withColumn("country",lit("India"))
df5.show()

df.groupBy("department").sum("salary").show()

df.show()

#count unique salary
df.select(approx_count_distinct("salary")).show()

#list all the values of salary column
df.select(collect_list("salary")).collect()

#list unique salaries
df.select(collect_set("salary")).collect()

#summ all values of salary column
df.select(sum("salary")).show()

#sum only unique salary
df.select(sumDistinct("salary")).show()